:project-name: Mallard
:imagesdir: ../assets/images/mallard
= {project-name} Load Store Unit (LSU) Micro-Architecture Specification (MAS)
Josh Smith <jsmith@sifive.com>; Mohit Wani <mohitwani@sifive.com>; John Ingalls <john.ingalls@sifive.com>; Monika Tkaczyk <monika.tkaczyk@sifive.com>
v4.0, {localdate}
:toc:
:toclevels: 5
//:xrefstyle: full
:sectnums:

== Revision History
[cols="1,3", options="header"]
|===
|Version    | Notes
|1.0        | U84
|2.0        | Mongoose release: P550
|3.0        | Narwhal release: P550
|4.0        | Orca release: P550, P650
|===

<<<
== Core Overview
Description of {project-name} Core can be found in the link below:

https://github.com/sifive/arch-specs/blob/master/core/mallard_mas.adoc[Mallard MAS]

<<<
== Load/Store Unit Overview
The Load/Store Unit (LSU) of {project-name} contains the Load/Store Issue Queue,
Load/Store Tag (LST) pipelines, L1 Data (LD) pipelines, Load Queue (LdQ) and Store Queue (StQ),
Miss Status Handling Registers (MSHRs) which also serve as both fill- and write-combining buffers,
Hardware Data Prefetcher (HWPF), and Memory Management Unit (MMU) Page Table Walker (PTW).

=== LSU functions
* Executes multiple loads / stores / prefetches / page table walks simultaneously.
* Buffers store addresses and values for retirement and forwards to loads.
* Prevents hazards violations: Read-After-Read (RAR), Read-After-Write (RAW),
Write-After-Read (WAR), Write-After-Write (WAW).
* Enforces barrier fences for weak order memory consistency.
* Coherently requests, allocates, writes-back, evicts, and invalidates cache entries
using the TileLink or CacheLink protocols.

=== LSU micro-architecture configuration
* L1 D-Cache Virtually Indexed Physically Tagged (VIPT) search in parallel with DTLB.
* L1 D-Cache Write-Back (WB) coherence protocol shared/unique clean/dirty
states M/E/S/I for large capacity and fast Atomic Memory Operations (AMO).
* L1 D-Cache serialized Tag lookup then Data read to reduce Data read power.
** 4-cycle latency from integer load to dependent operations in other integer execution pipelines.
** 3-cycle latency from load double-word to dependent address generation in LST Pipe.
** 5-cycle latency from misaligned or floating-point load.
* No duplicate information: no duplicate tags for VA/PA, no duplicate addresses in LdQ/StQ/MSHR.
* Area scaling for large LdQ/StQ: track hazard pairs (overhead O(n*log(n))) instead of full hazard matrix (overhead O(n^2)).

=== LSU generator parameters
* Number of Load/Store Tag (LST), Load Data (LD), and Store Data (SD) pipelines.
* Number of Issue Queue (LsIssQ) entries.
* Number of Load Queue (LdQ) entries.
* Number of Store Queue (StQ) entries.
* Number of Miss Status Handling Registers (MHSR).
* Number of Release Queue (RelQ) entries.
* L1 D-Cache capacity number of sets.
* L1 D-Cache associativity number of ways.
* L1 D-Cache block line size, default 64 bytes.
* L1 D-Cache data row beat width, from xLen to cacheBlockSize/2, inclusive, in powers of 2.
* L1 DTLB capacity number of entries.
* and more! See Chisel source code class `LSUParams`.

The fig. <<LoadStoreUnit>> shows a simplified organization of the Load/Store Unit.
[#LoadStoreUnit]
.Load Store Unit
image::Load_Store_Pipeline.png[]

== Glossary

=== Acronyms
[options="header"]
[cols="2*"]
[%autowidth]
|===
| Term    | Definition
| Addr    | Address[11:0] (where VA==PA)
| AGU     | Address Generation Unit
| ALU     | Arithmetic / Logic Unit
| AMO     | Atomic Memory Operation
| CCL     | CacheLink protocol
| CSR     | Control/Status Register
| D-Cache | Data Cache
| DFN     | Data Forwarding Network
| DIS     | Dispatch stage
| DTLB    | Data Translation Lookaside Buffer
| ECC     | Error Correcting Codes
| EX      | Execute stage
| FEX     | Floating point Execution unit
| FSM     | Finite State Machine
| GPR     | General Purpose Register
| HWPF    | Hardware Prefetch
| I-Cache | Instruction Cache
| IEX     | Integer Execution unit
| ISS     | Issue stage
| ITLB    | Instruction Translation Lookaside Buffer
| LDQ     | Load Queue
| LR      | Load Reserve
| LSU     | Load/Store Unit
| MDP     | Memory Dependence Predictor
| MMU     | Memory Management Unit
| MSHR    | Miss Status Handling Register
| PA      | Physical Address
| PC      | Program Counter
| PIPT    | Physically indexed, physically tagged
| PMP     | Physical Memory Protection
| PPN     | Physical Page Number
| PRN     | Physical Register Number
| PRF     | Physical Register File
| PTW     | Page Table Walker
| RAM     | Random Access Memory
| RAR     | Read-After-Read Hazard
| RAW     | Read-After-Write Hazard
| RelQ    | Release Queue
| RDU     | Rename-Dispatch Unit
| ROB     | Reorder Buffer
| RR      | Register Read stage
| RVTSO   | RISC-V Total Store Order memory model proposed extension
| SC      | Store Conditional
| SRAM    | Static Random Access Memory
| STLDF   | Store-to-Load Forwarding
| STQ     | Store Queue
| TL      | TileLink protocol
| TLB     | Translation Lookaside Buffer
| TLBI    | TLB Invalidate
| VA      | Virtual Address
| VPN     | Virtual Page Number
| WB      | Writeback stage
| WAR     | Write-After-Read Hazard
| WAW     | Write-After-Write Hazard
|===

=== Stages
[options="header"]
[cols="2*"]
[%autowidth]
|===
| Term     | Definition
| ISS      | Issue
| RR       | Register Read
| LST Pipe | Load/Store Tag Pipe
| LSTA     | LS Tag Address
| LSTR     | LS Tag Read
| LSTM     | LS Tag Match
| LSTO     | LS Tag Order
| LSTC     | LS Tag Confirm
| LD Pipe  | Load Data Pipe
| LDArb    | LD Arbitration
| LDR      | LD Read
| LDF      | LD Forward
| LDWB     | LD Writeback
| FLDWB    | Floating Point LD Writeback
| LDCF     | LD Confirm/Flush
| LDWR     | L1 Data Write
| LdQ      | Load Queue
| LdqRetAW | LdQ Return Arbitration Winner
| LdqWbReq | LdQ Writeback Request
| StQ      | Store Queue
| StqRetAW | StQ Retire Arbitration Winner
| StqMA    | StQ Memory Align
| StqWG    | StQ Write Gather
| MSHR     | Miss Status Handling Register
| ML1AW    | MSHR L1 Arbitration Winner
| ML2AW    | MSHR L2 Arbitration Winner
| TSR      | Tag Search Read
| TSM      | Tag Search Match
| TSW      | Tag Search Write
| MDR      | MSHR Data Read
| MDF      | MSHR Data Forward
| RelQ     | Release Queue
| TL_A     | TileLink A-Channel
| TL_B     | TileLink B-Channel
| TL_C     | TileLink C-Channel
| TL_D     | TileLink D-Channel
| TL_DB    | TileLink D-Channel Buffer
| TL_E     | TileLink E-Channel
| CCL_RH   | CacheLink Read Hint
| CCL_RD   | CacheLink Read Request
| CCL_WR   | CacheLink Write Request
| CCL_WB   | CacheLink Write Back
| CCL_DH   | CacheLink Data Hint
| CCL_DR   | CacheLink Data Response
| CCL_NR   | CacheLink Non-Data Response
| CCL_PB   | CacheLink Probe
| CCL_AM   | CacheLink Acknowledge from Master
|===

=== Actions
* *Establish*: When a queue entry is created / transitions to valid state.
Queues include ROB, LdQ, StQ, MSHR, RelQ.
* *Claim*: When a queue entry is reserved / picked off of the free list.
This may be a couple cycles _before_ Establishing the queue entry.
* *Resolve*: When an instruction determines that it will not punt-flush
or fault-flush, and reports this to the ROB.  Also known as 'Confirm'.
Doing this quickly is important for performance to Commit faster.
* *Commit*: See https://github.com/sifive/arch-specs/blob/master/core/mallard_mas.adoc#RobStatesCommitted[ROB Committed State].
Requires all older instructions to be Resolved, but not necessarily Complete.
Doing this quickly is important for performance to perform non-speculative Oldest Uncommitted operations faster.
* *Complete*: When a Load/AMO/SC returns data or a Store receives data.
Doing this quickly is important for performance to Retire queue entries faster.
* *Retire*: When a queue entry is freed / transitions to invalid state.
Requires all older instructions to be both Resolved and Complete.
Doing this quickly is important for performance to Establish new queue entries faster.
* *Flush*: When un-committed instructions need to be discarded and re-fetched.
It is important for performance to _avoid_ this if possible.
** A 'punt-flush' is when an instruction decides to flush itself and all newer
instructions, instead of resolving.
* *Replay*: When in-flight data needs to be discarded and re-executed.
This is less costly than Flush, but is limited in the window of time when this
can be signaled, otherwise the Issue queues would need to grow in size to track
many issued instructions that might replay.
* *Probe*: A TileLink B-Channel command from another agent that downgrades
this D-Cache's cache coherence state permissions or forces write-back.
Also known as 'snoop'.
* *Unroll*: When an operation holds in a pipe stage to spawn multiple beats into
the pipeline ahead of it.  For example, writing back a whole cache line requires
multiple data beats through the TileLink interface, or reading misaligned across
cache rows requires multiple data beats through the LDPipe.

== Load/Store Operations
The basic operation of loads and stores is as follows:

. Instructions are dispatched from the Rename-Dispatch Unit (RDU) into the Issue Queue.
. Issue Queue (LsIssQ) will issue instructions into the Load/Store Tag (LST) pipes to register read (RR) data from physical register file (PRF).
. Generate the virtual effective address (VA) in the AGU.
. Lookup the L1 DTLB to translate the virtual address to a physical address and check permissions.
. Lookup the L1 D-Cache Tag Array to determine which way (if any) is a hit for the physical address.
. Loads establish a Load Queue (LdQ) entry if they have hazards or do not immediately return data.
. Stores always establish a Store Queue (StQ) entry.
. Establish a Miss Status Handling Register (MSHR) if establishing a LdQ or StQ.
. Loads return data forwarded from the StQ, slow read from MSHR, fast hit from L1 D-Cache, or finally miss from TileLink.
. Stores issue a data piece as will into the Store Data (SD) pipes to register read (SDRR) data from physical register file (PRF),
then buffer data in the StQ until committed, then gather writes when retiring to L1 D-Cache or MSHR.
. MSHR makes miss requests to TileLink and gathers write-back data to TileLink.

A detailed version of the Load/Store Unit is shown in fig <<LoadStoreUnitDetailed>>.
[#LoadStoreUnitDetailed]
.Load Store Unit (Detailed)
image::Load_Store_Pipeline_Detailed.png[]

== Load/Store Tag+Data Pipeline Flow

.Load Execute Pipeline Diagram
image::Load_Execute_Pipeline_Diagram.png[]

=== Load/Store Tag (LST) Pipe

The Load/Store Tag (LST) Pipeline is responsible for generating, matching, and
ordering loads and stores addresses.

. *ISS* stage issues instructions from the Issue Queue when operand(s) are available.
* Issue picks the oldest instruction whose operand(s) are ready.
This is helpful for performance to reduce the number of older unknown hazards,
thus reducing the number of loads which require a LoadQ entry.
* All instructions issue with:
** ROB program order number for later use to resolve, complete, and/or flush.
* Load instructions issue with:
** destination Physical Register Number (PRN) for later use to return data.

. *RR* stage reads operand(s) from the Physical Register File (PRF).

. *LSTA* stage computes the effective virtual address (VA).
* Prefetches and Page Table Walks flow similarly to loads and mux in too during this stage.

. *LSTR* stage searches the DTLB and L1 D-Cache Tag array.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Virtual Address (VA).
* Unroll for cache-line-crossing misaligned access.
.. _Actions_ performed in this stage:
... Search the DTLB and L1 D-Cache:
* Read the L1 D-Cache Tag SRAM array.
* Search the DTLB Tag to lookup the Physical Address (PA) from DTLB Data.
* Loads read and search the L1 D-Cache Way Predictor.
... Compare Age and Addr[11:0] against the LdQ and StQ:
* Which are potentially hazardous (byte-offset-overlapping if PA match) older and newer LdQ entries and StQ entries.
* Which StQ entry is eligible newest older byte-overlapping VA-hash-matching to
steer Store-to-Load-Forwarding (STLDF) muxing next cycle in LDR stage.
* Load arbitrate for LD Pipe (LDR stage and L1dcData SRAM read port).
... Claim (pre-calculate) which LdQ, StQ, and/or MSHR entry to establish in the future if necessary.

. *LSTM* stage matches the LdQ, StQ, and MSHRs.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* L1 D-Cache Way Predictor Hit Way(s).
* L1 D-Cache Tag read ways.
* Physical Address (PA).
* Memory Management Page Permissions.
* DTLB Hit Way.
* Vectors of potentially hazardous (byte-offset-overlapping if PA match) older and newer LdQ entries and StQ entries.
* Claimed (pre-calculated) which LdQ, StQ, and/or MSHR entry to establish in the future if necessary.
.. _Calculated_ in this stage:
* Determine the L1 Hit Way by comparing the L1 D-Cache Tag PA versus the DTLB Data PA.
* Which MSHRs match this PA, and whether there is a Reusable MSHR matching this VAidx+PA.
* Which LdQ entries and StQ entries are PA-matching using Vector of MSHRs which match this PA.
* Fault due to permissions.
.. _Actions_ performed in this stage:
* DTLB replacement policy update.
* Early hint to CacheLink if L1 D-Cache Way Predictor Miss.

. *LSTO* stage orders the LdQ, StQ, and MSHRs.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Vector of MSHRs which this instruction is dependent on and ordered after.
* Vector of MSHRs which match this PA.
* Which Reusable MSHR matches this VAidx+PA, if any.
.. _Actions_ performed in this stage:
* Establish LdQ if necessary for reasons enumerated in LdQ description below.
* Establish StQ if Store, SC, or AMO instruction.
* Establish MSHR if establishing an LdQ or StQ as just described above and not reusing an existing MSHR.
* If newer hazardous LdQ or StQ entries are using a different MSHR, then flush and re-issue this instruction in-order.
** PTW LdQ entries (i.e. without a ROB index to compare age with) conservatively
compare as newer than all other ops with a ROB index.
* Fast request to LdqWbReq if this Load instruction is an L1 Hit, is not hazardous,
and did not enter the LD Pipe or did not read the right way in the LD Pipe.
* Fast request to TileLink Acquire if Load establishing a new MSHR with no blocking dependencies.

. *LSTC* stage confirms (resolves) or flushes to the ROB.
* All Store instructions resolve or flush from this stage, not from StQ.
* L1 D-Cache replacement policy update.
* L1 D-Cache way predictor fix update.

=== Load Data (LD) Pipe

The Load Data (LD) Pipeline is responsible for returning data to the
Data Forwarding Network (DFN) and Physical Register File (PRF).

It also serves as the L1 Data (L1D) Pipeline, which is responsible for reading
from and writing to the L1 Data Cache Data SRAM Arrays.

. *LDR* stage reads data.
* Parallel with *LSTM* stage if Load entered LD Pipe from LST pipe.
* _Sources_:
** L1 D-Cache Data SRAM array.
** Store-to-Load-Forwarding (STLDF) muxing data from StQ entries.
** TileLink D-Channel (TL_D) / CacheLink DR-Channel (CCL_DR) Data.
** MSHR Data Forward stage (MDF).

. *LDF* stage forwards data to consumers.
* Parallel with *LSTO* stage if Load entered LD Pipe from LST pipe.
* _Consumers_:
** Data Forwarding Network (DFN) to other execution pipelines if minimal alignment needed:
naturally aligned double-word (Addr[2:0]==0) or single-word (Addr[1:0]==0) with
either zero- or sign-extension.  If parameter "fastLoadByte"=true, then also byte
and aligned half-word (Addr[0]==0).
** If parameter "fastLoadForwardToLdStAddress" enabled: forward naturally-aligned
double-word (Addr[2:0]==0) to address computation in LSTA stage.
This helps performance to reduce the load-use latency for pointer chasing from 4 cycles to 3 cycles.
* _Actions_ performed in this stage:
** ECC Correction
** Mis-aligned data steering.

. *LDWB* stage writes Load data back to the Physical Register File (PRF).
* Parallel with *LSTC* stage if Load entered LD Pipe from LST pipe.
* _Actions_ performed in this stage:
** Atomic Memory Operation (AMO) ALU

. *LDWR* stage writes Read-Modify-Write (RMW) data to L1 D-Cache Data Array.
.. _Actions_ performed in this stage:
** ECC Generation
** L1 D-Cache Data Read-Modify-Write (RMW) for Atomic Memory Operation (AMO)
or ECC Update (correction or sub-ECC-granule partial write).

.Load Pipe L1 D-Cache Hit Fast
[cols="5*"]
[%autowidth]
|===
.2+h| Cycle 2+h| LST Pipe                                      2+h| LD Pipe
               | Stage            | Action                        | Stage              | Action
    | 1        | *LSTA* _Address_ | VA Adder                      |                    |
    | 2        | *LSTR* _Read_    | Dtlb lookup, L1dcTag search   |                    |
    | 3        | *LSTM* _Match_   | match LdQ, StQ, MSHR          | *LDR*  _Read_      | L1dcData read
    | 4        | *LSTO* _Order_   | Establish LdQ, StQ, MSHR      | *LDF*  _Forward_   | align, extend, forward
    | 5        | *LSTC* _Confirm_ | ROB: Resolve / Flush / Replay | *LDWB* _WriteBack_ | RegFile write
|===

=== Store Data (SD) Pipe

The Store Data (SD) Pipeline is responsible for reading data for store instructions
from the Physical Register File (PRF) and writing data into the StoreQ (STQ) entries.

The Store Data piece will issue via the separate Store Data issue port
two or more cycles after the Store Address piece, to line up at the earliest in both LSTO and SDO stages.
This is helpful for performance to reduce the number of older unknown hazards,
because a StQ entry with no data still informs loads in LSTO of a known hazard,
thus reducing the number of loads punt flushing due to unknown Read-After-Write (RAW) hazard.

. *SDISS* stage issues instructions store data piece from the Issue Queue
when its source data operand is available.
* Store Data Issue picks the oldest instruction whose operand is ready.
This is helpful for performance to Store-to-Load-Forward (STLDF) faster.
* Store Data Issue payload includes the ROB program order number
for later use to complete to the ROB, and match the already-issued address piece
for the store instruction in the LSTPipes or StoreQ entries.
* Parallel with *LSTA* stage if the store data piece issued as fast as possible.

. *SDRR* stage reads operand(s) from the Physical Register File (PRF).
* Parallel with *LSTR* stage if the store data piece issued as fast as possible.

. *SDFP* stage recodes Floating Point (FP) data.
* Parallel with *LSTM* stage if the store data piece issued as fast as possible.
* If scalar integer data, then can Store-to-Load-Forward (STLDF) from this stage.

. *SDO* stage fans out to StoreQ.
* Parallel with *LSTO* stage if the store data piece issued as fast as possible.

. *SDC* stage confirms (completes) to the ROB.
* Parallel with *LSTC* stage if the store data piece issued as fast as possible.
* All Store instructions complete from this stage, not from LSTC or StQ.


== Issue Queue (LsIssQ)

The Issue Queues are responsible for buffering instructions as they wait for input operands to
become ready.  A dispatching instruction will be allocated into one entry of the Issue Queue.
Once an entry's operands are all ready, and there are no issue blocking conditions, that entry will
set its ready bit and wait to be selected.

.Build-time Configuration Parameters
The Issue Queues are configurable at build time with a few parameters:

* The number of entries
* The number of dispatch ports into the queue (realistically expected to be 1 or 2)

==== Entry Allocation
The entry allocation policy is a free list. Note that if there are multiple
dispatch ports (numbered 0 to N-1), they will always be age-ordered such that the instruction on port 0 is
oldest, then port 1, etc...  This allows the Issue Queue to properly maintain information about the relative
age of entries.  Dispatch will never send instructions out of program order to the same Issue Queue. So when an entry
is allocated for a dispatching instruction, it is younger than all other valid entries.

.Find Alternating N from Free List
A bit-vector is used to keep track of valid (and allocated) entries, essentially a free list.
A find-first priority search is done on the bit vector to find an invalid entry.  Depending on the number
of configured dispatch ports, multiple find-first searches will be done.  The first search will be starting
from entry 0.  The second search will be starting from entry N-1 (for N entries).  A third search will
start from entry 1, and mask out the result of the first search.

When entries are released, the valid bits are cleared and the entry becomes available for dispatch again.

Note that we don't need to explicitly detect
cases where the multiple searches select the same entry, because near-full conditions will be handled by
the credit scheme described above, and dispatch will know not to send too many instructions.

==== Entry State
Each Issue Queue entry maintains state which allows the entry to track the readiness of operands, along with
other control bits and a payload used for decoding the instruction.

.Per-entry state
[options="header"]
[cols="1,3"]
|===
|Field      | Description
|valid      | Valid bit for entry
|ready      | Ready bit for entry
|older_vec  | Row of age matrix for this entry (only for oldest-ready configuration)
|payload    | Payload for execution
|srcValid[] | Bit per-source to indicate source is a register value (not immediate)
|srcReady[] | Bit per-source to indicate value of source will be ready (in PRF, bypassable, or immediate)
|srcPR[]    | Per-source PR index
|destValid  | Instruction writes to PRF
|destPR     | Destination PR Index
|===

==== Operand Wakeup
Each entry in the Issue Queue has logic to monitor the readiness of its input operands.
Each entry monitors tag broadcasts to determine when all input operands will speculatively be ready.  Readiness
may be speculative because the producing instruction may replay. When all operands are either known to be ready,
or speculated as ready, and no issue block conditions are in place, then the entry will set its ready bit
on the next cycle so that it may be selected for issue.

==== Instruction Replay
Because load instructions are speculated to hit in the DTLB and L1 D-Cache, we will speculatively wake up dependent
instructions.  These dependent instructions will be issued, and already in the execution pipe by the time
the load is determined to hit or miss.  In addition, these dependent instructions may in turn wake up more
instructions and cause them to be issued. In the event of a miss, the chain of dependent instructions must be replayed and
attempted again later when the miss is resolved.

To keep the replay simple, the Issue Queue entry allocated by an instruction will not be released until it is known
that the instruction will not replay.
The moment when it is known that an LS instruction will not replay is in the LSTO pipe stage, when the instruction has been able to establish all necessary resources (LdQ, StQ, MSHR, VLTB).

[#SrcTagBroadcastReplay]
.Operand Tag Broadcast Wakeup Replay Bypass
image::wakeup_replay.svg[]

==== Issue Blocking Conditions
There are certain scenarios in which we need to prevent an instruction from being issued by the Issue Queue,
generally due to structural hazards on shared resources such as PRF read or write ports.

.Possible Blocking Conditions
* DTLB miss wait for DTLB ready
* StoreQ claim available
* LoadQ claim available
* MSHR claim available
* L2hwpfQ claim available
* VLTB claim available
* PTW forward progress fairness
* Flush for containment after uncorrectable ECC error
* Non-speculative wait for ROB retire pointer
* In-order wait for all older entries to issue
* MDP wait for older predicted hazardous entries to issue

==== Instruction Selection
https://github.com/sifive/arch-specs/blob/master/core/mallard_mas.adoc#IssueDatapath[Issue Selection Datapath] illustrates the datapath for selecting an instruction to issue, the main difference
between the different configurations is in the logic to generate the mux select signals (not shown).

.Aged Select
The oldest-ready selection policy will guarantee that if multiple instructions are ready, the oldest in program order
will be selected first.  This is achieved with the use of the age matrix described in https://github.com/sifive/arch-specs/blob/master/core/mallard_mas.adoc#age-matrix.

The selection mux is implemented hierarchically with 2:1 muxes to achieve good timing.  For example, in the first level
of mux, the select signal only depends on the relative age of those two entries.  In the second level, only the relative
age among four entries needs to be in the cone of the select signal.  So the larger cones of logic are fed into the
later mux stages.  An example pseudo-code in verilog is presented below (assuming 4 entries, and just a single payload bit):

[source]
----
wire [3:0]       ready;
wire [3:0] [3:0] older;    // age matrix, older[y][x] means entry y older than entry x
wire [3:0]       payload;  // payload bit

// First level of 2:1 mux
wire [1:0] sel_lv1;    // select signals
wire [1:0] mux_lv1;    // mux outputs

assign sel_lv1[0] = (ready[1] & older[1][0]) | ~ready[0];
assign sel_lv1[1] = (ready[3] & older[3][2]) | ~ready[2];

assign mux_lv1[0] = sel_lv1[0] ? payload[1] : payload[0];
assign mux_lv1[1] = sel_lv1[1] ? payload[3] : payload[2];

// second level of 2:1 mux
wire sel_lv2;   // select signal
wire mux_lv2;   // mux output

assign sel_lv2 = ((ready[3] & older[3][1]) | (ready[2] & older[2][1]) | ~ready[1]) &
                 ((ready[3] & older[3][0]) | (ready[2] & older[2][0]) | ~ready[0]);

assign mux_lv2 = sel_lv2 ? mux_lv1[1] : mux_lv1[0];
----

=== Separate Store Data (SD) Issue
The Store Data piece will issue via the separate Store Data issue port
two or more cycles after the Store Address piece, to line up at the earliest in both LST and SD pipes.

=== Multiple Issue Select
The {project-name} LsIssQ supports multiple issue to multiple LSTPipes using
a unified and serialized picker that selects from all entries in any order.
The oldest ready entry is selected, then masked off from the input to the next
picker select, and so on, all in the same cycle.  This will become a timing
problem at larger capacities (32+ entries) and larger issue widths (3+).

The unified picker selects up to `nLSTPipes` oldest ready entries, in any mix
of loads and stores.  There is an additional limitation, however: once the picker
selects more than `nLDPipes` loads to issue, that and further instructions are blocked from issuing,
since it is better for performance for all loads to at least have a chance at
returning data via the fast path from LSTR to the LD Pipes.  Furthermore, when
this `nLDPipes` limitation is reached, those and later issue slots go unused,
i.e. the pick selection is _not_ changed to stores.

=== Operand Bypassing
In order to increase performance, instructions will be able to bypass input operands directly from various
stages of the execution pipeline.  Due to the difference in timing criticality of the different bypassing
sources, the bypass muxes are structured hierarchically in levels, with late-arriving inputs brought in
to later levels of muxes.  The diagram below illustrates the organization of the bypass muxing.  The most critical
signals are expected to be the single-cycle ALU results from EX stage, the load results, and the IRF read
data.

.Operand Bypass Datapath
image::operand_bypass.png[]


== Load Queue (LdQ)

The Load Queue (LdQ) is responsible for resolving hazards and/or returning data
for loads which were not able to do so from the LST Pipe.

=== LdQ Entries Management
* Loads which resolve (have no hazards) and complete (return their data)
from the LST Pipe will not establish an LdQ entry or MSHR entry.
This improves performance by reducing demand for LdQ/MSHR entries and thus pressure on LdQ/MSHR capacity.
* Load instructions claim LdQ entries out-of-order from a free list.
* The number of LdQ entries is parameterized, but expected to be in the range of 16 to 64.
Functionally there must be at least one entry for naturally aligned loads,
or two entries for cache-line-crossing loads.

.*LdQ Entry Lifecycle*
. Loads in LSTO stage establish an LdQ if they have hazards (cannot resolve)
or will not return data from LDWB stage (cannot complete).
* L1 cache state:
** L1 miss
** L1 hit but did not return data
* Hazards regardless of L1 state:
** Known Read-After-Read (RAR) where the older load did not yet return data.
** Known Read-After-Write (RAW) that did not STLDF.
** Unknown Read-After-Read (RAR)
** Unknown Read-After-Write (RAW)
. Known Hazards block data return.  Unknown Hazards configurably block data return.
. A Load may speculatively return data behind barriers by withholding resolve until
its MSHR's dependencies clear, and punt-flushing if Probed before its MSHRs dependencies clear.
. An LdQ entry retires after it completes (returns data) and resolves (its hazards retire).

[#LoadQueueFsmDiagram]
.LdQ Entry FSM Diagram
image::Load_Queue_FSM_Diagram.png[]

=== LdQ Hazard Tracking
* Every LdQ entry tracks both known and unknown hazards
Read-After-Read (RAR) and Read-After-Write (RAW).
* A simple and straightforward hazard tracking scheme is to maintain a vector in
every LdQ entry of every other LdQ and StQ entry that is a known hazard.
This poorly scales area overhead, but this optimally handles performance in
multiple-RAR and multiple-RAW scenarios, which should be rare but we don't know that for sure.
** If known RAR hazard on other LdQ entries _and_ reusing the _same_ MSHR,
then wait until the hazardous LdQ entries return data for this Load to resolve but not complete (return data).
If known RAR hazard on other LdQ entries _and_ reusing a _different_ MSHR,
then wait until the hazardous LdQ entries returns data for this Load to complete (return data) but not resolve.
** If known RAW hazard on StQ entries, then wait until the hazardous StQ entries
retire to complete (return data) and resolve.  If RAW hazardous on exactly one StQ entry that is
STLDF-eligible and fully satisfies this Load, then return data via STLDF
from that StQ entry to both complete and resolve.
* A more complex hazard tracking scheme is for every LdQ entry to track only
a single other LdQ entry and a single StQ entry that is a known hazard, and fall
back to a slower hazard resolution if multiple RAR or multiple RAW hazards exist.
This performs poorly in multiple-RAR and multiple-RAW scenarios, which should be
rare but we want to verify this via full-scale performance validation before implementing.
Selecting which of these two hazard tracking schemes should be parameterizable in the core generator.
** If known RAR hazard on one other LdQ entry _and_ reusing the _same_ MSHR,
then wait until the hazardous LdQ entry returns data for this Load to resolve but not complete (return data).
If known RAR hazard on one other LdQ entry _and_ reusing a _different_ MSHR,
then wait until the hazardous LdQ entry returns data for this Load to complete (return data) but not resolve.
** If known RAR hazard on multiple other LdQ entries, then remember the ROB order number
of the newest older hazardous Load and wait until that ROB order number is retired
for this Load to resolve.  This takes advantage of the axiom that
Loads will not complete before returning data, and the performance trade-off that
Loads are not commonly hazardous with multiple other Loads, to save area and not
track hazards from each LdQ entry to all other LdQ entries, thus scaling overhead
at O(n*log(n)) instead of full hazard matrix overhead O(n^2).
** If known RAW hazard on one StQ entry, then wait until the hazardous StQ entry
retires to complete (return data) and resolve.  If the hazardous StQ entry is
STLDF-eligible and fully satisfies this Load, then return data via STLDF
from that StQ entry to both complete and resolve.
** If known RAW hazard on multiple StQ entries, then wait until the newest older hazardous StQ entry
retires for this Load to complete (return data) and resolve.  This takes advantage
of the axiom that StQ entries retire in-order, and the performance trade-off that
Loads are not commonly hazardous with multiple older Stores, to save area and not
track hazards from each LdQ entry to all StQ entries, thus scaling overhead
at O(n*log(m)) instead of full hazard matrix overhead O(n*m).
* If unknown RAR or RAW hazard, then wait until the oldest load and store instruction
Issue Queue entries (which also covers the LST Pipe up through the LSTO stage)
ROB pointers pass this LdQ entry's ROB number.  The LdQ entry signals flush instead of resolve if:
** unknown Read-After-Read (RAR) hazard becomes known true RAR hazard _and_
this LdQ entry's MSHR is marked as Probed,
meaning that this Load returned data before the older Load gets possibly changed data.
** unknown Read-After-Write (RAW) hazard becomes known true RAW hazard,
meaning that this Load returned data before the older Store wrote it.

=== LdQ Data Return
. Every LdQ entry links to a MSHR, and refers to its MSHR to know when data is available
from L1 D-Cache or TileLink, or StQ entry for STLDF.
. Multiple LdQ entries requesting data return will arbitrate into the *LdqRetAW* stage,
picking the oldest requesting LdQ entry.
. *LdqRetAW* stage holds the arbitration winner for LdQ entry data return.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Which LdQ entry to mux from.
* Which MSHR to mux L1 D-Cache Hit Way and Addr[set] from.
.. _Actions_ performed in this stage:
* Mux information from LdQ entry and MSHR.
* To speed up load data return on long-latency misses, the oldest load needing data return
can speculatively wait in LdqRetAW stage, then enter LdqWbReq stage when TileLink D-Channel Data returns.
. *LdqWbReq* stage requests load data register write-back via the LD Pipe.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Physical Register Number (PRN) from LdQ.
* L1 D-Cache Hit Way and Addr[set] from MSHR.
* Which StQ entry to STLDF from.
.. _Actions_ performed in this stage:
* Load arbitrate for LD Pipe (LDR stage and L1dcData SRAM read port).
* Wake up dependent operations in other execution pipelines by broadcasting PRN advance ready.
. *LDR* stage: see earlier description.

.LdQ L1 D-Cache Hit slow read
[cols="3*"]
[%autowidth]
|===
.2+h| Cycle 2+h| LST/LD Pipe
               | Stage                  | Action
    | 1        | *LSTA* _Address_       | VA Adder
    | 2        | *LSTR* _Read_          | Dtlb lookup, L1dcTag search
    | 3        | *LSTM* _Match_         | match LdQ, StQ, MSHR
    | 4        | *LSTO* _Order_         | Establish LdQ, MSHR
    | 5        | *LSTC* _Confirm_       | ROB: Resolve / Flush / Replay
    | 6        | *LdQ##* _NeedsDataRtn_ | wait for data ready in L1
    | 7        | *LdQ##* _ReqDataRtn_   | request data return
    | 8        | *LdqRetAW* _ArbWinner_ | pick LdQ entry
    | 9        | *LdqWbReq* _WriteBack_ | LD Pipe arb, PRN wake-up dependents
    | 10       | *LDR*  _Read_          | L1dcData read
    | 11       | *LDF*  _Forward_       | align, extend, forward
    | 12       | *LDWB* _WriteBack_     | RegFile write
|===

.LdQ L2 Cache Hit
[cols="6*"]
[%autowidth]
|===
.2+h| Cycle 2+h| LST/LD Pipe                                            | CacheLink    | PL2     |
               | Stage                  | Action                        | Interface    | Stage   | Action
    | 1        | *LSTA* _Address_       | VA Adder                      |              |         |
    | 2        | *LSTR* _Read_          | Dtlb lookup, L1dcTag search   |              |         |
    | 3        | *LSTM* _Match_         | match LdQ, StQ, MSHR          | _RH-Channel_ | TM1     | read hint
    | 4        | *LSTO* _Order_         | Establish LdQ, MSHR           | _RD-Channel_ | T0      | read request
    | 5        | *LSTC* _Confirm_       | ROB: Resolve / Flush / Replay |              | T1      |
    | 6        | *LdqRetAW* _ArbWinner_ | wait for hint                 | _DH-Channel_ | T2, DM1 | data hint
    | 7        | *LdqRetAW* _ArbWinner_ | PRN wake-up LsIssQ dependents |              | D0      |
    | 8        | *LdqWbReq* _WriteBack_ | LD Pipe arb                   |              | D1      |
    | 9        | *LDR*  _Read_          | non-L1-data mux               | _DR-Channel_ | D2      | data return
    | 10       | *LDF*  _Forward_       | align, forward live to LSTA   |              | D3 Skid |
    | 11       | *LDWB* _WriteBack_     | RegFile write                 |              |         |
|===

.LdQ MSHR Hit or L1 D-Cache Hit fast re-read
[cols="5*"]
[%autowidth]
|===
.2+h| Cycle 2+h| LST/LD Pipe                                               2+h| MSHR
               | Stage                  | Action                              | Stage                  | Action
    | 1        | *LSTA* _Address_       | VA Adder                            |                        |
    | 2        | *LSTR* _Read_          | Dtlb lookup, L1dcTag search         |                        |
    | 3        | *LSTM* _Match_         | match LdQ, StQ, MSHR                |                        |
    | 4        | *LSTO* _Order_         | Establish LdQ, Reuse MSHR           | MDArb                  |
    | 5        | *LdqWbReq* _WriteBack_ | LD Pipe arb, PRN wake-up dependents | MshrData readout       | read MSHR Data
    | 6        | *LDR*  _Read_          | non-L1-data mux                     |                        | forward MSHR Data
    | 7        | *LDF*  _Forward_       | align, extend, forward              |                        |
    | 8        | *LDWB* _WriteBack_     | RegFile write                       |                        |
|===

.TileLink Slow
[cols="5*"]
[%autowidth]
|===
.2+h| Cycle 2+h| LST/LD Pipe                                               2+h| MSHR/L2
               | Stage                  | Action                              | Stage                  | Action
    | 1        | *LSTA* _Address_       | VA Adder                            |                        |
    | 2        | *LSTR* _Read_          | Dtlb lookup, L1dcTag search         |                        |
    | 3        | *LSTM* _Match_         | match LdQ, StQ, MSHR                |                        |
    | 4        | *LSTO* _Order_         | Establish LdQ, MSHR                 |                        |
    | 5        | *LSTC* _Confirm_       | ROB: Resolve / Flush / Replay       | *TileLink* _A-Channel_ | Acquire
    | 6        | *LdQ##* _NeedsDataRtn_ | wait for TL Grant MSHR data beat    | *TileLink* _D-Channel_ | GrantData
    | 7        | *LdQ##* _ReqDataRtn_   | request data return                 | *TL_DB* _Buffer_       |
    | 8        | *LdqRetAW* _ArbWinner_ | pick LdQ entry                      | MDArb                  | write MSHR Data
    | 9        | *LdqWbReq* _WriteBack_ | LD Pipe arb, PRN wake-up dependents | MshrData readout       | read MSHR Data
    | 10       | *LDR*  _Read_          | non-L1-data mux                     |                        | forward MSHR Data
    | 11       | *LDF*  _Forward_       | align, extend, forward              |                        |
    | 12       | *LDWB* _WriteBack_     | RegFile write                       |                        |
|===

.LdQ Entry Fields
Entry contents: see Chisel source code class bundle `LSTOtoLdQEntryEstablish`.

To reduce area overhead, only keep Addr[5:0] and ByteCountLog2[1:0] to calculate byte overlap,
and instead rely on MSHR entries Physical Address (PA) to completely detect hazards.

== Store Queue (StQ)

The Store Queue is responsible for committing the stores in program order,
then retiring data to update the L1 D-Cache and/or write back to TileLink via MSHR.

=== StQ Entries Management
* Store instructions claim StQ entries out-of-order from a free list.
* The number of StQ entries is parameterized, but expected to be in the range of 16 to 32.
Functionally there must be at least one entry for naturally aligned stores,
or two entries for cache-line-crossing stores.

.*StQ Entry Lifecycle*
. All Stores in LSTO stage establish a StQ entry at their claimed index.
. All Stores resolve, complete, and/or flush in the LSTC stage, not the StQ.
. StQ entries are established only by the Store instruction's address piece
in the LSTO stage, then the corresponding ROB order number's
Store Data piece will issue via the separate Store Data issue port
at the same time or any later time as the Store Address piece.
This is helpful for performance to reduce the number of older unknown hazards,
because a StQ entry with no data still informs loads in LSTO of a known hazard,
thus reducing the number of loads punt flushing due to unknown Read-After-Write (RAW) hazard.
. Stores commit and retire regardless of L1 D-Cache Hit State.
Every StQ entry links to a MSHR, and relies on its MSHR to guarantee that data
will eventually and correctly become globally observable, either allocated in
the L1 D-Cache and/or written back to TileLink.
. StQ entries retire in-order into the *StqWG* stage.
StQ entries wait to retire until they are committed _and_ their data is guaranteed
to be behind older hazardous loads from the same hart (WAR hazard), i.e. the
Store instruction is retired or next-to-retire in the ROB.
. Newer loads from the same hart track RAW hazard on both the StQ
Entry and StqWG stage until the data is written to a place that does not need
Read-After-Write (RAW) hazard tracking:
into the L1 D-Cache Data array for L1 hits and/or MSHR Data array for L1
misses or hits without L1 coherence permission Modified state (dirty).

.StQ Entry Fields
Entry contents: see Chisel source code class bundle `LSTOtoStQEntryEstablish`.

To reduce area overhead, only keep Addr[5:0] and ByteCountLog2[1:0] to calculate byte overlap,
and instead rely on MSHR entries Physical Address (PA) to completely detect hazards.
If more precision is desired for STLDF, can add some VA bits partial or hashed to StQ.

[#StoreQueueDataDiagram]
.StoreQ Data Diagram
image::Store_Queue_Data_Diagram.png[]

=== StQ Hazard Tracking
* StQ entries do not track hazards, but instead rely on the axiom that
Loads will not complete before returning data (as in RVTSO),
and the axiom that StQ entries retire in-order.
** MSHRs may still write back out-of-order, to take advantage of
weakly ordered memory consistency model for performance.

.*Store-to-Load-Forwarding (STLDF)*
Normal Cacheable Stores are eligible for Store-to-Load Forwarding (STLDF) except when...

* the Store or Load are to an MMIO memory region (i.e. not Normal Cacheable).
* the Store and Load use different MSHRs (including across a Fence).
* the StQ entry was established by either Atomic Memory Operation (AMO) or Store Conditional (SC) instruction.

=== StQ Data Retirement
. *StqRetArb* stage calculates the arbitration winner for StQ entry retirement.
.. _Actions_ performed in this stage:
* Pick StQ entry to mux from.  This is the winner of the AgeMatrix arbitration.
* Mux from StQ entry: data, alignment steering offset, and which MSHR.
. *StqMA* stage performs memory alignment.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Data and alignment steering offset.
* Which MSHR to mux L1 D-Cache Hit Way, Addr[set], and coherence state from.
.. _Actions_ performed in this stage:
* Steer bytes from register width to data row width.
* Hold in this stage if unable to gather or move into the next StqWG stage.
* Retire StQ entry (i.e. clear entry valid) when moving into the next StqWG stage.
. *StqWG* stage performs write gathering within a data row.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Which MSHR is in use.
* L1 D-Cache Hit Way, Addr[set], and coherence state from MSHR.
* Memory-aligned data row and byte enables accumulated using the same MSHR.
.. _Actions_ performed in this stage:
* Hold in this stage to gather data until StqMA uses a different MSHR or data row than StqWG,
or the StQ is empty.
* If holding then byte-wise mux data bypass reads that overlap, or if moving then write the enabled bytes to:
** L1 D-Cache Data Hit Way and Addr[set,offset] if
L1 coherence permission Valid state (Shared, Exclusive, or Modified).
If writing bytes less than the ECC granularity then first read the existing ECC granule via the LD Pipe.
** MSHR Data index and Addr[offset] if L1 D-Cache miss or hit without
L1 coherence permission Modified state (dirty).

[#StoreQueueRetireFsmDiagram]
.StQ Entry Retire FSM Diagram
image::Store_Queue_Retire_FSM_Diagram.png[]

== L1 D-Cache (L1DC)

The L1 D-Cache is a set-associative VIPT cache, meaning that it is indexed purely with
virtual address bits VA[set] and tagged fully with all translate physical address bits PA[msb:12].
For low power consumption, only one data SRAM way is accessed.
The L1 D-Cache line size is 64 Bytes, and the beat size is 16 or 32 Bytes to match TileLink bandwidth.
Each load/store pipeline will read up to 8 Bytes of data per cycle from a single cacheline.
Misaligned load/store is supported.

=== L1DC Coherence
The L1 D-Cache Write-Back (WB) coherence protocol includes shared/unique clean/dirty
states M/E/S/I for large capacity and fast Atomic Memory Operations (AMO).

Table <<CoherenceProtocolsComparison>> shows a handy mapping between contemporary nomenclatures.
[#CoherenceProtocolsComparison]
.Comparison of Coherence Protocols and States
[options="header"]
[cols="5*^"]
[%autowidth]
|===
| {project-name} D-Cache | MOESI     | ACE               | TileLink  | Rocket D-Cache
| Modified               | Modified  | Unique Dirty      | Tip Dirty | Dirty
| Exclusive              | Exclusive | Unique Clean      | Tip Clean | Trunk
| n/a                    | Owned     | Shared Dirty/Last | Trunk     | n/a
| Shared                 | Shared    | Shared Clean      | Branch    | Branch
| Invalid                | Invalid   | Invalid           | Nothing   | Nothing
|===

=== L1DC Tag Array
Entry contents: see Chisel source code class bundle `L1DCTagMetaReq`.

The Tag Array is read and written from these stages with this arbitration priority:

. LSTA read: load/store instruction search
. TSW write: Reset and Probe/Evict
. MDF write: L1DC Allocate
. Tag Search arbitartion (TSArb) read: search for Probe/Evict
. PTW/HWPF read: search for page table walk / before hardware prefetch

If any of the LSTPipes' RR or LSTA stages are valid, than a write to the L1dcTag Array
will stall LSISS to create a single bubble cycle in the pipeline for the Tag Write.

=== L1DC Way Predictor
The Way Predictor is an array which contains one entry per cache block in the L1 Data Cache.
Instead of tagging each entry with physical address like the L1DC Tag Array,
the Way Predictor is tagged by a hash of the virtual address.
This allows us to make a prediction about which way will hit without having to look up in the DTLB.
The Way Predictor is physically implemented as an SRAM and is not ECC-protected.

Entry contents:
[options="header"]
[cols="1,3"]
[%autowidth]
|===
|Field         | Description
|Valid         | Entry Valid
|VAHash        | VA Hash tag
|===

The Way Predictor is read in LSTR stage, and all ways are compared against the hash of the Virtual Address.
The hit vector is then used to generate the enable for each way in the
Data Array.  This means that we will read at most one way from the Data Array per access.  This path from reading
the Way Predictor, to the comparison and index generation, to the Data Array read index is expected to be
one of the most timing critical paths in the LSU.

The Way Predictor is written from these stages with this arbitration priority:

. TSW: Reset and CFlushAll
. MDF: L1DC Allocate
. LSTC: Mis-Predict Update

Writes take priority over reads.

=== L1DC Data Array
Entry contents: data bytes, grouped into ECC granules plus optional ECC bits,
grouped into datapath granules for precise power saving read enabling.

==== Misaligned
Accesses to any alignment of consecutive bytes within a cache line up to one-half
the data row width is possible in a single cycle by controlling the address row
separately for the even (lower) and odd (upper) halves of each data row.

Example 64-byte cache line size, 32-byte data row size, load double-word
misaligned address 'd28, odd half select row=0 / even half select row=1:
[options="header"]
[cols="32*"]
[%autowidth]
|===
16+^| `addrBeatRowOdd=0` 16+^| `addrBeatRowEven=1` |
{set:cellbgcolor:#FFFFFF} 63|62|61|60|59|58|57|56|55|54|53|52|51|50|49|48|47|46|45|44|43|42|41|40|39|38|37|36|
{set:cellbgcolor:#00FFFF} *35*|*34*|*33*|*32*|*31*|*30*|*29*|*28*|
{set:cellbgcolor:#FFFFFF} 27|26|25|24|23|22|21|20|19|18|17|16|15|14|13|12|11|10|09|08|07|06|05|04|03|02|01|00
|===

Misaligned loads do not require adding any pipeline stages, but just are not
eligible for fast-forwarding from the first data return stage (LDF) and thus
incur one cycle of additional latency.

Read/write across cachelines, and across pages of the same memory type, requires
unrolling the second half of the access from the LSTR stage, which will then
establish a second LoadQ/StoreQ entry (and MshrEntry if required).
Data merge across cachelines is finally done in the LD Pipe stage LDWB.
As there is only one LDWB stage buffer "LDUB" (Load Data Unroll Buffer),
there is a structural hazard on this single entry, that an unrolled Load across
cachelines must wait for all older unrolled loads and block all newer unrolled
loads, so that no other unrolled Loads pieces can get between each other.

==== Read-Modify-Write (RMW)
Certain operations on the L1 D-Cache require Read-Modify-Write (RMW) operations.
These include:

* Atomic memory operations.
* Stores with data width less than the ECC granularity (byte and halfword).
* ECC correctable errors detection then recovery.

These operations are supported by the LD Pipe, where these single RMW operations
now access the cache twice, in two different cycles.
This imposes extra structural hazards on the pipeline, the results of which
are bypassed for performance to newer operations coming down the pipeline.

== ECC Errors
Both the L1 D-Cache Tag Array and the L1 D-Cache Data Array are protected by ECC:
Single Error Correct, Double Error Detect (SECDED).
TileLink responds with Corrupt and Denied errors.

Errors report to the Bus Error Unit (BEU) to signal an interrupt, not a precise exception.
However, we do try our best to make the interrupt as precise as possible to contain uncorrectable errors:

* on an uncorrectable data error,
we punt-flush-refetch after the offending instruction if it has not yet resolved (committed)
so that we will take the interrupt before the error propagates through subsequent instructions,
but we will not execute the memory access twice.
** If an instruction is already resolved (committed) then we cannot be precise anymore,
although we do have a mode (chicken) bit to force loads to wait to resolve until they return data 'forceLoadResolveWaitDataReturn'.
* on an uncorrectable tag error, we punt-flush-refetch the offending instruction itself, before the memory access,
so that we will take the interrupt before the error propagates from that instruction.

The various error cases and handling are outlined below.

=== L1DC Tag Errors
The readout tag will be corrected inline in the path to LSTM and TSM stages, but not in
the path to LDR stage and L1dcData SRAM due to physical implementation timing concern.

Correctable Tag Error Recovery is similar to a way mis-predict:

* If the correctable tag error masked an L1 Hit, then
the LST Pipe will recognize that the LD Pipe is not reading the right way,
and will re-request data return via the LdQ and/or LdqWbReq fastpath from LSTO stage.
* If the correctable tag error masked an L1 Miss, then
the LST Pipe will recognize that the LD Pipe should not read the L1DC at all,
and will request data return via the LdQ from LSTO stage.
* TBD whether to fix correctable errors in the array.

Un-correctable Tag Error Containment is necessary to limit data corruption: the cache may hold
dirty data and it's not possible to tell which memory address it corresponds to.
If the un-correctable tag error happens on ...

* an instruction or a Prefetch, then throw away the search and punt-flush-refetch the instruction.
* a PTW, then treat the search as a miss.
* a Probe or Evict Release in TSPipe, then treat the search as a miss.

The Tag Array ECC flow is shown in diagram <<TagArrayECC>>
[#TagArrayECC]
.Tag Array ECC Data Flow
image::Tag_Array_ECC_simplified.png[]

The error decoding logic is duplicated in LSTPipe so as to have it continue
independently of that of TSPipe.

=== L1DC Data Array Errors
The readout data will be corrected inline in the path from LDF stage to everywhere
except for DFN and LSTA stage due to physical implementation timing concern.

Correctable Data Error Recovery is similar to a way mis-predict:

* If a correctable data error is forwarded from the LDF stage to DFN or LSTA,
it signals replay to its dependencies in the next cycle from the LDWB stage,
and writes the corrected data to the PRF for its dependencies to read after re-issuing.
* Store instruction with data width less than the ECC granularity (byte and halfword),
then simply correct the erroneous data in-line before writing the L1 D-Cache Data Array.
* Store instruction with data width a multiple of whole ECC granularity (words),
then simply over-write the erroneous data because we don't even check ECC in this case.
* Corrected data essentially gets forwarded from LDF stage for writing to DCache, to PTW, and to AMO.
* TBD whether to fix correctable errors in the array, which we would do by read-modify-write.

Uncorrectable Data Error Containment is necessary to limit data corruption.
If the un-correctable data error happens on a ...

* Load instruction signaling resolve or flush to ROB (ldqNdxValid=1), then punt-flush-refetch the instruction.
* Load instruction that has not yet signaled resolve to ROB (ldqNdxValid=0), then punt-flush-refetch the instruction.
* Load instruction or PTW that already signaled resolve to ROB (ldqNdxValid=0), then we cannot be precise anymore.
* Store instruction with data width less than the ECC granularity (byte and halfword),
or AMO instruction, then write the data array with poisoned ECC encoding.
* Store instruction with data width a multiple of whole ECC granularity (words),
then simply over-write the erroneous data because we don't even check ECC in this case.
* Probe or Evict Release in TSPipe, then mark the data beat as 'corrupt' to TileLink.

Table <<ECCActions>> summarizes the actions that the LSU will take in case of ECC errors.
[#ECCActions]
.ECC actions
[options="header"]
[cols="3*"]
[%autowidth]
|===
|Data Array scenarios                                        | Correctable Errors                    | Uncorrectable Errors
|Load Instruction: ldqNdxValid=0 (cannot signal to ROB)      | Fix inline + Replay fast dependencies |Asynchronous interrupt fatal + Replay dependencies
|Load Instruction: ldqNdxValid=1 (ready to resolve or flush) | Fix inline + Replay fast dependencies |Asynchronous interrupt fatal + Replay dependencies + Flush before (punt)
|Store Instruction: Width less than ECC granularity          | Fix inline                            |Asynchronous interrupt fatal + Poison data
|AMO                                                         | Fix inline                            |Asynchronous interrupt fatal + Poison data
|PTW                                                         | Fix inline                            |Asynchronous interrupt fatal
|Writeback Dirty for Probe or Evict (Release)                | Fix inline                            |Asynchronous interrupt fatal + Poison data
|===

The relation between these actions and what instructions they affect is described in table <<LSUXcptTypes>>
[#LSUXcptTypes]
.LSU exception types
[options="header"]
[cols="5*"]
[%autowidth]
|===
|Action | Dependents | Newer | Self | Trap
|Replay | Re-Issue|||
|Flush After 2+^| Re-Fetch Next ||
|Flush Before (punt) 3+^|Re-fetch Self |
|Sync Exception 4+^|Flush Newer, Trap Self
|Async Interrupt||||Interrupt later
|===

The Data Array ECC flow is shown in diagram <<DataArrayECC>>
[#DataArrayECC]
.Data Array ECC Data Flow
image::Data_Array_ECC_simplified.png[]

* Data Array Rows contain 1-or-more Datapath Granules with separate read enables,
to save power from reading the array.
* Datapath Granules contain 1-or-more ECC Granules with separate write enables,
so that a full-ECC-Granule write does not need to read-modify-write in the array.
* ECC Granules contain SECDED encoding bits and 1-or-more data bytes.
A partial-ECC-Granule write must read-modify-write (RMW) to update the ECC for the
combined old and new data bytes within the ECC granule.

To illustrate the data array row granules nesting layout, see diagram <<DataPathGranules>>
[#DataPathGranules]
.DataPath Granules
image::Data_Path_Granules.png[]

=== TileLink Errors
TileLink corrupt and denied errors behave similarly to an Uncorrectable L1 Data Error.
If the TileLink error happens on a ...

* Load instruction signaling resolve or flush to ROB (ldqNdxValid=1), then punt-flush-refetch the instruction.
* Load instruction or PTW not signaling resolve or flush to ROB (ldqNdxValid=0), then we cannot be precise anymore.
* Store or AMO instruction, or Prefetch, then write the data array with poisoned ECC encoding.

== Miss Status Handling Register (MSHR)

The MSHR is responsible for servicing cache misses, TileLink requests, L1 allocation,
write-combining store buffer, cache coherence, and memory ordering consistency.

=== MSHR Entries Management
* LST Pipe operations (Load, Store, Prefetch, PTW) claim MSHR entries from a free list.
* The number of MSHR entries is parameterized, but expected to be in the range of 8 to 32.
Functionally there must be at least one entry for naturally aligned memory access,
or two entries for cache-line-crossing memory access.

=== MSHR Entry Lifecycle
. Establish from LSTO stage if: Load or PTW establish LdQ, Store establish StQ, or Prefetch missed L1.
. MSHRs to Normal Cacheable memory regions may be reused _except_ across Fences,
after matching Probes, or after Retirement Request due to MSHR full.
. MSHRs established by PTW can _only_ be reused by other PTW, and vice versa,
PTW can only reuse MSHRs established by PTW, because PTW is a separate memory agent from Load/Store.
. A MSHR retires after it has no LdQs or StQs using it, its L1 and L2 FSMs are idle,
it has no dirty data, and all of its older dependencies are cleared.

[#MshrVldFsmDiagram]
.MSHR Entry Valid FSM Diagram
image::MSHR_Valid_FSM_Diagram.png[]

.MSHR Entry Fields
Entry contents: see Chisel source code class bundle `MSHREntrytoLSTMCompare`.

==== MSHR Reusable
MSHR entries are established as Reusable for access to Normal Cacheable memory except when...

* to a MMIO memory region (i.e. not Normal Cacheable).
* established by a Fence instruction.
* established by either Atomic Memory Operation (AMO) or
Load Reserve / Store Conditional (LR/SC) instruction.

MSHR entries Reusable state is cleared when...

* matched by a Probe (snoop).
* matched by the predecessor set of a newer Fence instruction.
* matched by any newer access that establishes a new MSHR entry instead of reusing.
* forward progress flush from LSTO stage.

==== MSHR Write Gathering
MSHR entries act both as a Fill Buffer for L2 requests data response and
as a Write Combining Buffer for Stores.
Generally, store latency is not critical for performance, and multiple stores
should be combined into as few transactions as possible.
However, it may take time to allocate dirty data into the L1 Data Cache or
write through dirty data to the L2 Cache.
So, there is a balance to be struck between waiting to gather versus eagerly freeing entries.

MSHR entries will wait to write their dirty data until...

* All loads and stores are done to this entry, and either this entry is non-reusable or
the number of dirty entries reaches or exceeds the configurable threshold "mshrCleaningThresholdEntries".
* This entry is reusable and the Cleaning Timer has ticked,
to make stores visible in a finite amount of time.

=== Memory Consistency Ordering
The execution on {project-name} core guarantees only weak memory ordering consistency
according to the RISC-V Weak Memory Ordering model (RVWMO).
The execution on {project-name} core takes advantage of _nearly ALL_ of the
allowed re-orderings in RVWMO, including but not limited to this list:

* Newer loads can read before older loads.
* Newer loads can read before older stores.
* Newer stores can write before older stores.

With the exception of this one allowed re-ordering:

* that Newer stores could write before older loads.
** The StoreQ micro-architecture on {project-name} enforces that
Newer stores do *not* write before older loads,
but instead must wait for all older loads to retire (both commit and return data).
This is an area optimization so the StoreQ does not need to track write-after-read
hazards on the LoadQ, and correspondingly does not need to track write-after-write
hazards on other entries within the StoreQ.

A combination of chicken bits is available to enforce a stricter memory ordering consistency model,
but at the cost of possibly substantially reduced performance, as we have not optimized
the {project-name} micro-architecture for such a mode, although we could in the future
for the RISC-V Total Store Ordering model (RVTSO).

=== MSHR Dependency Tracking
* MSHRs to different addresses in Normal Cacheable memory regions have no dependencies
on each other for either reads or writes,
to take advantage of weakly ordered memory consistency model for performance.
* MSHRs to the same address maintain coherence order (byte-wise single-writer/multiple-reader)
by tracking dependencies on older MSHRs to the same physical address.
* MSHRs to MMIO memory strongly ordered regions maintain sequential consistency
within that region by tracking dependencies on all older MSHRs to that MMIO memory region.
* MSHRs to MMIO memory global strongly ordered regions maintain sequential consistency
by tracking dependencies on all older MSHRs to all MMIO memory regions.
* The Fence instruction issues in-order and sets MSHR dependencies to provide the requested memory ordering.
A MSHR for a Fence instruction has dependencies on older MSHRs in the _predecessor_ set.
MSHRs in the Fence's _successor_ set have a dependency on the Fence's MSHR.
Multiple fences may be outstanding in MSHRs.
* A MSHR's L1 and L2 FSMs are blocked until the MSHR's dependencies clear, i.e. retire.

=== MSHR L2 FSM

The MSHR L2 FSM is responsible for sending TileLink requests and waiting for responses.

See the https://github.com/sifive/tl-spec[TileLink specification] for messages and channels.

.*TileLink A-Channel*
Arbitration is by MSHR Least Recently Used ordering,
as a performance optimization that older requests should complete first.

* Acquire and Get (read) requests directly enter the TileLink A-Channel from ML2AW stage.
* PutData (write) requests unroll into the MSHR Data Pipe from ML2AW stage then to TileLink A-Channel.
* Requests hold in ML2AW stage if matching Release Queue (RelQ) entries (and may be
replaced in ML2AW stage if holding) because TileLink requires that
"A master should not issue a Release if there is a pending Grant on the block.
Once the Release is issued, the master should not issue ProbeAcks, Acquires, or further
Releases until it receives a ReleaseAck from the slave acknowledging completion of the writeback."

.*TileLink D/E-Channels*
GrantAck responses are necessary for forward progress in the TileLink network,
so they separately arbitrate and request from MSHRs to the TileLink E-Channel.
MSHR receiving GrantData will respond GrantAck immediately, for performance.
The {project-name} MSHRs functionally support Probes matching outstanding Grants,
so the GrantAck message is functionally unnecessary for {project-name},
only for complying with the TileLink-Cacheable protocol.

[#MshrL2FsmDiagram]
.MSHR Entry L2 FSM Diagram
image::MSHR_L2_FSM_Diagram.png[]

=== MSHR L1 FSM

The MSHR L1 FSM is responsible for L1 D-Cache Allocation, Eviction, and Updates.
The ML1AW stage requests either the TS Pipe or MD Pipe.

* MSHRs whose L1 Allocation will replace a dirty entry must first evict that entry using the TS Pipe.
* L1 Allocation (Invalid to Valid) unrolls data beats in the MSHR Data Pipe (MDR stage + MshrData SRAM).
* L1 Allocation Updates (Shared/Exclusive to Modified) does not unroll data beats.

Stores writing to cache blocks in the Shared or Exclusive state will behave like a
write-through L1 cache: both update L1 data and later TileLink Acquire
Exclusive state and upgrade L1 to Modified, or if probed then PutPartial to L2.

[#MshrL1FsmDiagram]
.MSHR Entry L1 FSM Diagram
image::MSHR_L1_FSM_Diagram.png[]

The algorithm for a MSHR allocating and evicting L1 Data Cache entries follows this sequence:

. Load miss in LST Pipe establish MSHR and request L2.
.. By default, store misses do not allocate the L1 D-Cache, though this is configurable via CSR setting.
. After L2 request accepted, but before response: if all ways are valid in the cache set,
then pick a target way to allocate.  If the target way contains an entry in the
Modified state then we must coherently write back the dirty data with a TileLink 'ReleaseData' message.
.. The default L1 D-Cache replacement policy is Pseudo-Least-Recently-Used (PLRU),
of the flavor with the number of bits = nWays-1, but other policies can be substituted,
including Least-Recently-Used (LRU) or Pseudo-Random.
.. Software may configure at runtime whether we should also send a TileLink 'Release'
message before overwriting a valid clean (Shared or Exclusive) entry by setting
the lsuChickenCSR bit 'forceTLReleaseWhenEvictClean'.  This enables subsequent
inclusive caches to more precisely track which cache lines are present inside the L1 D-Cache.
. The MSHR Entry requests ML1AW stage to request TagSrch pipeline to evict the entry at the target way from TSW stage.
.. the Evict from Modified to Invalid actually only needs to go down to Shared
. Each MSHR Entry continually updates the 'target' way steps 2-3 with coherence state changes
and PLRU updates to the whole set for as long as the MSHR Entry is valid.
.. Each MSHR remembers for its cache set both the coherence state MESI for all ways and the PLRU for the set.
.. PLRU updates are from the LSTC stage (all non-faulting memory operations) and the MDF stage (allocation).
. After L2 response data received, the MSHR Entry requests ML1AW stage to request
MshrData pipeline to allocate the entry from MDF stage. If the 'target' way shifted
between steps 3-5 to a dirty way, then cancel the allocation and repeat at step 3.
If the L1 Allocation discovers in the MDF stage that its target way is dirty,
then it cancels the allocation and the MSHR entry will request eviction of that
entry targeted for replacement via the TS Pipe.

=== MSHR Data

The MSHR Data array is physically implemented as a synchronous register file,
dual-ported (1 read, 1 write) if possible for performance.

The MSHR line size data storage matches the L1 D-Cache block line size,
and the data row beat width matches L1 and TileLink bandwidth.
So in an example with 64 byte cache lines and 16 bytes data row beat width,
the MSHR Data array is indexed by MSHR index and Address[5:4].

Every MSHR has an entry in the MSHR Data array and tracks byte-wise dirty-ness.

.*MSHR Data Arbitration*
The MSHR Data Array is read and written from these stages with this arbitration priority:

. L2 data response write (can never hold, i.e. TileLink D-Channel `ready` is always true)
. MDR read: unroll (back into itself)
. then, round-robin among these four, starting in this order:
.. StqWG write: stores
.. ML2AW read: L2 request PutData
.. LdqRet read: LoadQ return data
.. ML1AW read: L1DC Allocate
. then, lastly, LSTO read: Load fast-path return data

If separate read and write ports are provided, such as with a dual-port
SRAM array `dpsramMshrDataArray`,or when building the MSHR Data Array
using flops (i.e. `spsramMshrDataArray==dpsramMshrDataArray==false`)
then reads and writes in the above list are split into separate read
and write arbitrations.

The MSHR Data Array supports banking by the parameter `nMshrDataBanks`.
The bank select bits are the least significant bits (LSBs) of the MSHR Entry Index.
If multiple banks are provided, then still only one of the MDR,
ML2AW, or ML1AW stages can read at a time, because these three stages
also must arbitrate for the single MSHR Data Pipeline's MDR stage.
But, the LdqRet and LSTO stages can read simultaneously with each other
and those three stages using the MDPipe, up to the number of banks
configured, because the LdqRet and LSTO stages do not use the MDPipe.

.*MSHR Data ECC*
The MSHR Data array is not ECC protected because it is more akin to a register file
than a cache, i.e. does not hold dirty data for long term storage.

The MSHR Data array is byte-write-able such that store-byte/halfword do not need to read-modify-write.

If in the future we wish to protect the MSHR Data array with ECC, or we wish to
consolidate the write granularity, then first read the whole write granule via the MD Pipe for read-modify-write.

== Tag Search pipe (TagSrch)

The Tag Search pipeline (TagSrch) is responsible for _downgrading_ cache lines
coherence state, either when responding to a received TileLink Probe message or
when Evicting a cache line sending a TileLink Release message.
Furthermore, when downgrading a Modified cache line, we must also write back the
Dirty data from the L1 D-Cache Data Array by reading via the L1 Data Pipeline (LDPipe).

. *TSR* stage access the L1 D-Cache Tag array.
.. _Sources_:
* TileLink B-Channel: Probe.
* ML1AW: MSHR Search and Eviction.
.. _Actions_ performed in this stage:
... Search the L1 D-Cache:
* Read the L1 D-Cache Tag SRAM array.
* Determine the L1 Hit Way by comparing the L1 D-Cache Tag PA.
... Compare PA against the MSHRs.

. *TSM* stage matches the search results.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* L1 D-Cache Hit Way.
* Vector of matching MSHR entries.
.. _Calculated_ in this stage:
* Which RelQs match this PA.
* Update vector of matching MSHR entries.
* Claim (pre-calculate) which RelQ to establish next cycle if necessary.
* Decide next cycle actions: hold or write L1DCTag / request TileLink C-Channel / request LD Pipe.

. *TSW* stage writes L1DCTag.
.. _Available_ from a flop in this pipeline stage after calculation in the previous stage:
* Vector of matching MSHR entries.
* Vector of matching RelQ entries.
* L1DCTag write enable and way.
.. _Calculated_ in this stage:
* Update vector of matching MSHR entries.
* Claim (pre-calculate) which RelQ to establish next cycle if necessary.
* Decide next cycle actions: hold or write L1DCTag / request TileLink C-Channel / request LD Pipe.
.. _Actions_ performed in this stage:
* If the Probe discovers in the TSM stage that it is a hit, then
if the over-written cache line's coherence permission state is:
** MSHR using the cache line (hit set+way) or MSHR Physical Address match then mark
those MSHRs as Probed (non-L1-Allocating, non-Reusable).
** Matching a Release Queue (RelQ) entry then set state in that RelQ entry to
later respond with ProbeAck[Data] after the RelQ entry receives ReleaseAck,
rather than immediately responding from TSW stage to TileLink C-Channel, because TileLink requires that
"Once the Release is issued, the master should not issue ProbeAcks, Acquires, or further
Releases until it receives a ReleaseAck from the slave acknowledging completion of the writeback."
** Shared or Exclusive state then respond TileLink C-Channel ProbeAck message.
** Modified state then unroll into LD Pipe to respond TileLink C-Channel
ProbeAckData message because the cache block is Dirty.
* If a Release discovers in the TSM stage that the entry to be evicted is:
** in use by a MSHR pending Acquire then do nothing because TileLink requires that
"A master should not issue a Release if there is a pending Grant on the block".
** in the Shared or Exclusive state with no MSHR pending Acquire then write the
L1 D-Cache Tag but no TileLink Release transfer is required because the cache block is Clean.
** in the Modified state then a TileLink ReleaseData transfer is required because the cache block is Dirty.

.Probe Evict Dirty
[options="header"]
[cols="3*"]
[%autowidth]
|===
| Cycle | Stage                  | Action
| 1     | *TileLink* _B-Channel_ | Probe
| 2     | *TSR* _Search_         | L1dcTag search, MSHR match
| 3     | *TSM* _Match_          | plan eviction next cycle
| 4     | *TSW* _Write_          | LD Pipe arbitration.  when won: L1dcTag write, MSHR update.
| 5     | *LDR* _Read_           | L1dcData read
| 6     | *LDF* _Forward_        | L1dcData forward
| 7     | *LDWB* _WriteBack_     | *TileLink* _C-Channel_ ProbeAckData
|===

=== Release Queue (RelQ)

The Release Queue (RelQ) is responsible for enforcing this TileLink ordering rule:
"Once the Release is issued, the master should not issue ProbeAcks, Acquires, or further
Releases until it receives a ReleaseAck from the slave acknowledging completion of the writeback."

.*RelQ Entries Management*
* TS Pipe cache line evictions claim RelQ entries from a free list.
* The number of RelQ entries is parameterized, but expected to be in the range of 4 to 16.
Functionally there must be at least one entry.

.*RelQ Entry Lifecycle*
. Establish from TSW stage if cache line eviction sends TileLink Release.
. RelQs may be reused by one (and only one) Probe, as TileLink interface
promises that "Once the Probe is issued, the slave should not issue further
Probes on that block until it receives a ProbeAck."
. A RelQ retires after it receives ReleaseAck response from TileLink D-Channel,
and if necessary sends ProbeAck[Data] to TileLink C-Channel.

[#RelQFsmDiagram]
.RelQ Entry FSM Diagram
image::Release_Queue_FSM_Diagram.png[]

=== Virtual Address Synonym Alias disambiguation

Virtual Address Synonym Aliases are disambiguated through the L2 back-invalidating
via TileLink.  Different VA[setIdxMsb:12] are sent to TileLink as if from different
virtual L1 data cache master agents, so the L2 will coherently back-invalidate when
the same Physical Address (PA) is accessed from a different Virtual Address (VA) cache set index.

=== Cache Ops

SiFive custom cache operations are defined at https://github.com/sifive/arch-specs/blob/master/cflush-cease.adoc.

CFlushAll (CFLUSH.D.L1 with rs1==x0) establishes a MSHR which requests TSPipe.
The TSR stage includes an FSM to unroll an L1DC Eviction op for every set-and-way.

CFlushLine (CFLUSH.D.L1 with rs1!=x0) establishes a MSHR if LSTO is a L1DC hit,
which then evicts the entry as normal through the TSPipe.

== L2 Cache Interface

=== TileLink (TL)
Originally in U84, {project-name} LSU used TileLink (TL) to communicate with the
Composable Cache as L2 (CL2).

https://github.com/sifive/arch-specs/blob/master/interconnect/tilelink_spec.adoc[TileLink Spec]

=== CacheLink (CCL)
Starting in P550, {project-name} added a Private L2 Cache (PL2) inside the core tile,
which the LSU communicates with using CacheLink (CCL), compatible with TileLink.

https://github.com/sifive/arch-specs/blob/master/core/privateL2_mas.adoc[PL2 and CCL MAS]

== Forward Progress Arbitration Rules

When deciding arbitration priority order ranking rules, we balance these functional
requirements and performance optimizations:

* Functional: operations which complete in a finite amount of time are best allowed to finish atomically,
i.e. unrolling within a pipeline stage should not be interrupted.
* Functional: TileLink requires that channels with lower priority do not indefinitely block
channels with higher priority: i.e., TileLink A-Channel valid-and-not-ready
does not cause any other channel ready to be low, and TileLink B-Channel ready is
necessary to make forward progress to satisfy A-Channel requests.
* Functional: older instructions should not be blocked by younger instructions.
* Functional: slow fall-back paths should not be blocked by fast paths.
* Performance: Loads should return data quickly to help resolve data-dependent branches.
* Performance: Stores can be buffered off to the side.
* Performance: L1 Data Cache Allocation helps return Loads data faster and reduces the number of LoadQ Entries required.

To guarantee that we do not deadlock, we specify this arbitration priority,
in descending order from highest to lowest:

. TileLink E-Channel
. TileLink D-Channel
. TileLink C-Channel / TSW
. TileLink B-Channel
. MD Pipe (Mshr Data): completes in a finite amount of time.
. StqWG (StoreQ retire): contains instructions which are committed and retired from the ROB.
. ML2AW / TileLink A-Channel: serves StqWG above and LdqWbReq below.
. LdqWbReq (LoadQ retire): may contain instructions which are committed and ready to return data but not yet retired from the ROB.
. PTW: contains instructions which are not yet committed.
. ML1AW: second-to-last because this is not functionally required for forward progress.
. LST: last because this fast path is not functionally required for forward progress.

Any performance-enhancing predictor schemes that deviate from this arbitration
priority order _MUST_ include a fall-back mechanism to revert to this safe
arbitration priority order within a finite bounded amount of time.

=== Overlapping events around L1 D-Cache

The MDF (MSHR Data Forward) and TSW (Tag Search Write) stages must enforce
atomicity on allocations and evictions, respectively.

The LD Pipe does not hold and bypasses updated data from stages down the pipeline,
so it is safe to follow an active operation in that pipeline,
and it is also safe to wait for such an active operation to finish.

The priority for accessing a L1 D-Cache entry (set+way) is:

. MD Pipe (MDR, MDF) allocating or updating because an operation unrolling in
MDR must atomically complete.
. TSW (probe or release) because it is important to make forward progress on these.
. To ensure fairness, forward progress, and atomicity without long propagated hold paths,
TSW and MD Pipe (MDR, MDF) causes stages (NextLDPipe: LdqWbReq, StqWG; NextMDPipe: ML1AW)
to hold that use L1 D-Cache entries hit coherence state from MSHRs.

== Signaling Flushes and Replays

For performance, flushes should be avoided; replaying an instruction is a
much lower penalty than flushing it.

The LSU Issue Queue (LsIssQ) is responsible for buffering instructions from dispatch
through execution in the LST Pipe, including replays and back-pressure up to
and including the LSTO stage.

The prioritization and blocked actions of flushes and replays are:

. If an instruction is replayed due to a dependency on another instruction,
either a source operand from an execution/data pipeline or to maintain in-order
issue in the LST Pipeline, then the instruction is killed immediately in
whichever pipeline stage it is in.
. _else_ If an instruction is punt-flushing itself or taking an exception, then
it will continue down the LST Pipeline to report the exception to the ROB from
the LSTC stage, but will not establish or act on any queues and will not return data.
. _else_ If an instruction is flushing all instructions newer than itself, then
it will continue down the LST Pipeline as normal and additionally make a
"flush-newer" request to the ROB from the LSTC stage.
. _and/else_ if an instruction is replaying itself then it will not establish
or act on any queues and will not return data.
. _and/else_ if an instruction is replaying its dependents then it will not return data.

An instruction will _also_ replay its dependents to prevent speculative execution
if it is punt-flushing itself, taking an exception, replaying itself, or flushing newer.

=== Exception for Permissions

Upon detection of a DTLB or PMA fault in LSTM stage, the instruction will report
an exception from the LSTC stage.
See <<DTLB Fault Handling>> for further details outside the LST Pipe.

Atomic operations (AMO/LR/SC instructions) and load/store to IO memory address
regions with side-effects must still be aligned and will thus check for
misaligned access exception.  If any piece of an unrolled misaligned load or store
touches IO memory address regions with side-effects, then that piece will report
a misaligned access exception from the LSTC stage, otherwise only the last piece
of an unrolled misaligned load or store reports resolve or complete.

=== Replay for DTLB Miss

Upon detection of a DTLB miss, the LSTM stage will request the PTW and
will replay itself back to the issue queue until the PTW is idle.
See <<DTLB Miss Handling>> for further details outside the LST Pipe.

Justification: The IssueQ is better suited to hold all of the instruction information
than the LoadQ or StoreQ to re-search the DTLB (and L1 D-Cache) after a DTLB miss.

=== Replay for L1 D-Cache Miss

The LD Pipe Arbitration winner (conceptually the LdArb stage) wakes up dependents
two cycles before data forwarding will be ready if possible from the LDF/LDWB stages.
This wake-up is speculative and will need to replay dependents if:

* LDR is reading the wrong L1 D-Cache entry, such as when the access parallel
in LSTM is a miss or way-prediction selected the wrong way.
* LDF is in parallel with LSTO and LSTO detected a hazard which should block
data return.  For timing, this replay may need to be captured into and signaled
from the LDWB stage.

=== Replay/Flush for LoadQ (LdQ) capacity

If the LoadQ is full when LSTO tries to establish a LoadQ entry, then LSTO will:

. replay itself in the IssQ to wait for re-issue until the LoadQ is not full, _and_
. flush the newest LoadQ Entry with a valid ROB index (i.e. not PTW)
in the NeedsResolveFlush state, if there is one newer than this load, or this load is a PTW.
. optionally by chicken bit, flush all instructions newer than itself.

Justification: Loads may claim a variable number of LdQ Entries if they could be misaligned.

=== Replay/Flush for StoreQ (StQ) capacity

If the StoreQ is full when LSTO tries to establish a StoreQ entry, then LSTO will:

. replay itself in the IssQ to wait for re-issue until the StoreQ is not full, _and_
. if the StoreQ already contains a newer store than this store,
then flush all instructions newer than itself.

Justification: Stores may claim a variable number of StQ Entries if they could be misaligned.

=== Replay/Flush for MSHR capacity

If the MSHR is full when LSTO tries to establish a MSHR entry, then LSTO will:

. if there is a newer LoadQ Entry in the NeedsResolveFlush state, then
.. flush the newest LoadQ Entry with a valid ROB index (i.e. not PTW)
in the NeedsResolveFlush state, _and_
.. replay itself in the IssQ to wait for re-issue until the LoadQ is not full,
just like the LoadQ full case above (likely immediately).
. _else_ otherwise then
.. replay itself in the IssQ to wait for re-issue until the MSHR is not full, _and_
.. flush all instructions newer than itself if there exists a
newer LoadQ Entry with a valid ROB index (i.e. not PTW) or StoreQ Entry, _and_
.. clear the Reusable bit in all MSHRs to guarantee forward progress if there is not a
newer LoadQ Entry with a valid ROB index (i.e. not PTW) or StoreQ Entry.
. optionally by chicken bit, flush all instructions newer than itself and clear the Reusable bit in all MSHRs.

Justification: It is preferable to keep as many already-issued instructions as
possible, so we will gradually flush the newest newer possible instruction until
a MSHR entry frees up.

=== Flush for MSHR ordering

If an instruction establishes a MSHR with a dependency on an _older_ MSHR
which is used by a _newer_ instruction or a PTW (i.e. no valid ROB index)
then the establishing instruction must flush the newer instruction(s)
to break the MSHR dependencies out of a deadlock.

This scenario arises when issuing instructions out-of-order, and may be
prevented to improve performance by predicting whether to issue instructions
that pathologically hit this scenario.

The {project-name} initial implementation will flush all newer instructions
in this scenario, but a future optimization could be to precisely flush only
the exact newer instruction(s) using the older MSHR(s).

=== Replay after Unroll

An unrolled load or store may partially replay to the LsIssQ for a DTLB Miss
from LSTM or LdQ/StQ/MSHR capacity claim full from LSTO.  When this happens,
execution resumes from the unrolled piece, not from the beginning.

== Memory Dependence Predictor (MDP)
The Memory Dependence Predictor (MDP) is tasked with predicting memory dependencies between stores and loads.
Without the MDP, a store and a younger load are allowed to execute out of order.   If the load is found to be memory
dependent on the store, i.e. the two instructions access the same memory location, a high performance penalty of
flushing and re-executing the load and all younger instructions is incurred.  By predicting such dependencies and
forcing ordering between appropriate loads and stores, the MDP can minimize performance degradation from flushes and
increase performance by allowing independent loads and stores to execute out of order.

The MDP operates assuming that loads and stores that have encountered memory ordering violations in the past are
likely to exhibit the same dependencies in the future.  The predictor groups dependent loads and stores into sets,
as it's possible for a load to read a location written by multiple stores and for a store to write a location read
by multiple loads.  Note that the many to one relationship between loads and stores in a set does not need to
be exhibited every time instructions are executed.   The MDP marks all loads and stores in a set with a
Dependence Set ID (DSID) that allows it to know precisely which loads should wait on which stores.
The DSID can be, but does not need to be unique to each dependence set.  Since the DSID is stored in the MDP
table as well as other structures, the smallest range of DSIDs (with fewest number of bits to encode) that
offers an acceptable level of aliasing between sets should be used.

=== MDP Organization
The main component of the MDP is a table that records information about loads and stores that have previously
caused flushes due to memory ordering violations.   Specifically, each entry in the table corresponds to either
such a load or store.  The table is addressed using a hash of instruction's PC.  It is fully associative with
configurable size.  Each entry contains the following information:

* PC tag
* Instruction type: LD or ST
* Set type: single or multiple (loads only)
* DSID
* Usefulness Counter (usefulCnt)

The exact allocation into the MDP table is described in the state transition section, however the general idea
is as follows.  When a RAW hazard flush occurs, the MDP attempts to capture in its table information about both
the load and store involved.  If neither instruction is currently present in the table, both get allocated into
separate entries and are assigned a newly generated DSID to indicate they belong to the same set.  If only one
operation is present in the predictor, the missing operation allocates a new entry and inherits the hitting
entry's DSID. In other words, the new instruction joins the already recorded instruction's dependence set.
If both operations are already present in the predictor, then their entries are appropriately updated according
to their current state.

=== Load/Store Serialization by MDP
Once information about dependent loads/stores is recorded in the MDP, the predictor attempts to serialize them
to prevent future ordering violations.  Loads and stores in the DIS stage that are to be dispatched to the LS issue
queue look up the MDP table.  If a hit is detected, instructions retrieve their DSIDs and prediction hysteresis
information.  As MDP hitting stores are dispatched, they simply record their DSIDs in the issue queue.  MDP hitting
loads, however, must find whether there are stores from the same dependence set ahead of them that they must
serialize with. Depending on the size of the MDP table, these loads will most likely not have enough time to
compare their DSIDs against those of stores currently dispatching or already in the issue queue.   For this reason,
MDP predicted loads will not be able to issue immediately after dispatch if there are any MDP hitting stores in the
queue.   In the cycle following dispatch, each load can precisely determine for which stores it should wait.
If there are no stores in the queue with the same DSID, the load will be free to issue.  If there is only a single
store with the same DSID, the load will issue after that store.  If there are multiple stores in the queue with the
load's DSID or the load is marked to belong to multiple dependence sets, it will wait for all MDP predicted stores
to issue.  We have chosen not to record which exact stores a load is waiting on in case of multiple hitting stores
to limit the number of required storage bits per issue queue entry.

The earliest a predicted dependent load can issue is in the cycle after the dependent store.  A side-by-side issue
would require MDP hitting loads to inherit the corresponding store's address register dependency and to rely on it
for a wakeup.  Issuing memory dependencies side-by-side is most likely not worth this complexity.

TBD  If the performance penalty from unnecessarily holding up MDP hitting loads in the issue queue is too large,
the MDP table could be read in the REN stage.  This would require extra read ports to the table to lookup all renamed
instructions or extra logic to skid MDP lookups when more LS instructions are renamed than dispatched.

=== MDP Training
There are three types of events that cause MDP training: memory RAW hazard violations, useful or useless
serializations of loads/stores based on MDP predictions, and periodic cleanup of MDP's unused entries.  The mechanisms
behind these training types are described in sections below.

A new memory ordering violation is detected when a LSTO store conflicts with a LdQ entry allocated by a younger load.
As the LdQ entry detects this violation, it stores the StQ number of the conflicting store.  The LdQ FSM transitions
to the Request Resolve/Flush state and arbitrates among all entries requesting to resolve. When the entry becomes
oldest ready to resolve, the StQ is read to retrieve the store's ROB ID and then a flush is requested from the ROB.
In the ROB, the request to train is written into a small buffer until corresponding load and store PCs are read out
of the ROB. The training buffer monitors retiring entries and captures PCs of those that are present in the MDP
training buffer.  Retire is currently back pressured so that only one ROB entry referenced by the training buffer
retires per cycle to minimize logic required to capture PCs. Once the PCs of instructions to train are known, the MDP
is updated in RMW mode. In some cases, it is possible that either the store or load ROB entry is retired and
overwritten before the hazard is signaled or required PCs are read out.  In those cases, the training of the predictor
is dropped.  Similarly, training events can be dropped if the training buffer is full.

The MDP, as any predictor, benefits from training based on the accuracy of its predictions.  When a load is serialized
due to a MDP prediction, we can approximate how useful the serialization was by looking at StQ RAW hazards the load
sees in LSTO stage.  If RAW hazards are found, then the load was likely held back correctly.  If no hazards are found,
the serialization was most likely unnecessary and detrimental.  All LSTPipes communicate confirmation information to
the MDP training buffer in the ROB.  Specifically, if a load hit on any older StQ entry, then the load and the youngest
hitting store request training.  If a load did not hit on any StQ entry, then only the load requests training.
Additionally, a bit is propagated through the pipe for each op predicted by MDP to indicated whether the op's
counters are saturated and the op does not need to train in case of a correct prediction.

The third type of training required by MDP is periodic decreasing of usefulness counters to prune out entries
that are no longer in use. This will eliminate entries for loads/stores that do not get executed, stores that g
et executed but no longer block any loads, and loads that no longer encounter any blocking stores.

=== MDP Entry States
This section describes states and state transitions for MDP entries.
Abbreviations used are: LD - load, ST - store,  HF - hazard flush (RAW), DSID - dependence set ID, SLD - single
set load, MLD - multi set load

==== Load States and Transitions
.Invalid (usefulCnt==0)
* Transition to SLD if HF.  Assign ST's DSID if ST is in a valid state.  Assign new DSID if ST is Invalid.

.Single Set Load (SLD)
* LD waits for ST with same DSID if only one such ST is present in issue queue.  If multiple STs with same DSID,
then LD waits for STs from all sets
* Increment usefulCnt if useful serialization with ST with any DSID
* Decrement usefulCnt if unnecessary serialization with ST.  This might cause transition to invalid
* Increment usefulCnt if HF with ST with same DSID
* Transition to MLD if HF with ST with different DSID.  Change LS's DSID to ST's DSID
* Transition to invalid if usefulCnt decays to zero

.Multi Set Load (MLD)
* LD waits for STs with all DSIDs
* Increment usefulCnt if useful serialization with ST with different DSID (Might also want to increment for same DSID
  and at higher rate for different DSID.  The idea is to transition MLD to SLD if it really needs to belong to a single set only)
* Increment usefulCnt if HF with ST with any DSID
* Decrement usefulCnt if unnecessary serialization with ST with any DSID.  This might cause transition to SLD
* Transition to SLD if usefulCnt decays to zero

==== Store States and Transitions
.Invalid (usefulCnt==0)
* Transition to ST if caused HF.  Assign new DSID if LD is invalid, otherwise assign LD's DSID

.Store (ST)
* ST blocks SLDs with same DSID and any MLDs
* Increment usefulCnt if useful serialization with LD with any DSID
* Increment usefulCnt if caused HF of LD with any DSID
* Transition to invalid if usefulCnt decays to zero
* The rate at which usefulCnts are incremented for useful serialization might have to be higher than the rate at which
usefulCnts are decremented for unnecessary serialization.  This is because the penalty from unnecessary serialization
is on average lower than from a flush due to violated ordering.

Additionally, it might be beneficial to add  "Weak" and "Strong" classifications to valid predictor states that would
correspond to the values of the usefulness counters above or below a certain threshold.   Weak states allow the
predictor to track information about previously discovered dependent loads and stores without forcing them to be
serialized.  Example behavior for the weak states is shown below.

.Weak Single Set Load (WSLD)
* LD does not wait for any STs
* Increment usefulCnt if useful serialization with ST with same DSID.  Counter increment might cause transition to MLD
* Decrement usefulCnt if unnecessary serialization with ST with same DSID
* Transition to SLD if HF with ST with same DSID or with invalid ST.
* Transition to SLD if HF with ST with different DSID.  Change DSID to ST's

.Weak Store (WST)
* ST does not block any LDs
* Increment usefulCnt if useful serialization with LD with any DSID.  Counter increment might cause transition to ST
* Decrement usefulCnt if unnecessary serialization with LD with same DSID
* Transition to ST if caused HF of LD with same DSID or with invalid LD
* Transition to ST if caused HF of LD with different DSID.  Change DSID to LD's if SLD or MLD.  Assign new DSID if WSLD

== Atomic Memory Operations (AMO)

=== Cacheable Atomics

The {project-name} L1 D-Cache uses a Write-Back (WB) coherence protocol including
Modified state for fast internal Atomic Memory Operations (AMO).

Atomic Memory Operations behave like a Load-Update operation.  Because the Update
is a side-effect that must be Committed and Ordered like a Store, the Update action
waits until the Load-Update is the Oldest Unretired instruction.
Since the AMO returns pre-Update data, the AMO StQ is _not_ eligible for STLDF,
but the AMO Load action _is_ eligible for speculative load data return.

. The AMO instruction establishes a LdQ entry for Load data return,
StQ entry for atomic operand data (linked as STLDFwdable), and Non-Reusable MSHR.
.. The LdQ entry waits to be committed _and_ for StQ entry to have data
to request data return completion, then waits in LdqWbReq stage for
MSHR to get the cache line L1 coherence permission Modified state, not Exclusive,
because the cache line must be guaranteed to become Dirty.
. The LdQ entry enters the LD Pipe and performs an atomic read-modify-write in
the LDF stage using the atomic operand data STLDF from StQ entry.
.. The LdQ entry will punt-flush if Probed after speculatively returning old data.
.. Forward progress may be guaranteed by blocking speculative AMO data return and
by structurally forwarding the Update data to dependent operations (e.g. ProbeData) in the LD Pipe.
. The StQ entry retires but does not write data from StqWG.

A Cacheable AMO that is Probed (and thus no longer allocate-able into the L1DC)
will change its behavior to that of a Non-Cacheable AMO (after the MSHR L1 and L2 FSMs are Idle).
Atomics that were probed to Branch (Shared) will invalidate L1 before sending Atomic to L2.

=== Non-Cacheable Atomics

TileLink provides A-channel commands for Arithmetic and Logical Atomic Memory Operations (AMO).

. The AMO instruction establishes a LdQ entry for Load data return,
StQ entry for atomic operand data (_not_ linked as STLDFwdable), and Non-Reusable MSHR.
. The StQ entry waits to be committed (instead of the usual retired from ROB) to
write the MSHR, then the MSHR makes a TileLink AMO request.
. The MSHR receives one beat of response data, just like a MMIO Load, then the
LdQ returns the data from the MSHR.

=== Load Reserve / Store Conditional (LR/SC)

. LR/SC instructions issue in-order.
. the LR+SC instructions establish separate non-reusable MSHRs because the separate
instructions can have separate Fence ordering semantics.
. the Load Reserve instruction waits for commit to set the reservation (set+way) in the LD Pipe.
.. for speed, the Load Reserve instruction may speculatively resolve and return data,
but withhold completion until committed and only set the reservation (set+way) in the LD Pipe
if the MSHR was not Probed.
. the Store Conditional instruction behaves almost like an atomic Load-Update: it
establishes a LdQ entry for Condition Pass/Fail data return, StQ entry for store data,
and a Non-Reusable MSHR.
.. the SC's Load action may speculatively return Condition Pass if the load reservation
is still set in the LD Pipe.
.. the SC's Store action may non-speculatively return Condition Fail if the load reservation
is no longer set in the LD Pipe.
. The LdQ entry waits to be committed _and_ for StQ entry to have data
to request data return completion, then waits in LdqWbReq stage for
MSHR to get the cache line L1 coherence permission Modified state, not Exclusive,
because the cache line must be guaranteed to become Dirty.
. The LdQ entry enters the LD Pipe and performs a conditional write in
the LDF stage using the store data STLDF from StQ entry.
.. If the load reservation (set+way) in the LD Pipe is still set (indicating that no Probes
or other memory accesses intervened), then the SC's Load piece returns Condition Pass
and writes Update data into the L1 D-Cache, otherwise returns Condition Fail
and does not write data.
... The SC's LdQ entry will punt-flush if returning Condition Fail after
speculatively returning Condition Pass.
.. Forward progress may be guaranteed by blocking speculative SC data return and
by structurally forwarding the Update data to dependent operations in the LD Pipe.
. The StQ entry retires but does not write data from StqWG.

== L1 DTLB
The L1 DTLB is a structure used to cache virtual-to-physical address translations. The
L1 DTLB is organized the same as the open-source Rocket DTLB.

=== DTLB Miss Handling
Upon detection of a DTLB miss, a request will be sent to the PTW and the instruction
will replay itself back to the issue queue.
If the PTW is not ready, the request will be dropped and the instruction will
request again after re-issuing when the PTW finishes.

Upon receiving a response from the PTW, the data will be written to the way specified by the
replacement policy, and any operations in the issue queues that were waiting for the PTW are woken up.

.DTLB Miss to PTW timing diagram
image::DTLB_Miss_PTW_Diagram.png[]

=== DTLB Fault Handling
Upon detection of a DTLB fault, the faulting operation is marked as an exception.
The exception operation will continue down the pipeline, but will not update any architectural state.
An exception will be signaled to the ROB, including the ROB group number, an appropriate
cause encoding, and the address which generated the fault.  If the instruction makes it
to the head of the ROB, an exception will then be taken. The mechanism for taking an exception
in general is detailed in https://github.com/sifive/arch-specs/blob/master/core/mallard_mas.adoc#Exceptions[Exceptions].

== Memory Management Unit (MMU)
This section provides more detail on the virtual-to-physical address translation hardware in the {project-name} LSU.

Description of {project-name} MMU can be found in:
https://github.com/sifive/arch-specs/blob/master/core/mallard_mmu_mas.adoc[Mallard MMU MAS]

=== Page Table Walks (PTW)
The Page Table Walker (PTW) will inject memory requests for PTEs into the LSTR stage,
including the L1 D-Cache Tag Array, arbitrating versus the LSTA stage and Data Prefetcher.
PTWs will establish MSHRs that are re-usable by other PTWs, but no other ops.

=== TLB Invalidations (TLBI)
SFENCE.VMA instructions will be sent down the pipeline like a store after waiting
to be oldest uncommitted in the LsIssQ.  The LSTC stage sends an invalidation
request to the PTW and L1 TLBs, and provide the values of rs1 and/or rs2 as necessary.

== Hardware Prefetcher

Description of {project-name} HWPF can be found in the link below:

https://github.com/sifive/arch-specs/blob/master/core/hwpf_mas.adoc[HWPF MAS]

Configurable Prefetch Engines train from matching loads in the LST Pipe.
The Prefetch Engines inject Prefetch Operations into the LSTR stage,
searching L1 D-Cache Tag Array and DTLB, arbitrating versus the LSTA stage and PTW.
Prefetch Ops will establish MSHRs that are re-usable by Loads and Stores.

== Chicken Bits
The {project-name} LSU includes a software-programmable CSR to tune or defeature the core,
colloquially referred to as chicken bits. This chicken CSR has ID 0x7c2.
See Chisel source code class bundle `LSUChickenBits` for contents.
